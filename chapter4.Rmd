# **Assignment 4: Clustering and classification**

In this analysis I will use the Boston data set available at https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html. The data contains information on 14 variables describing the housing values in 506 suburbs of Boston. Here I will focus on the per capita crime rate (crim) and study if there is correspondence to any other variable. To conduct this analysis, different methods to cluster and classify the observations are tested.

```{r include=FALSE}
# access needed packages
library(MASS); library(tidyr); library(corrplot); library(dplyr)
library(ggplot2); library(GGally)
```

```{r}
# load boston data and check contents
data("Boston")
summary(Boston)
```

Distributions and correlations of different variables are shown below. The distribution of crime rates resembles logarithmic distribution with mostly very low crime rates. The distributions of other variables are also skewed. This may cause problems when linear discriminate analysis and clustering is applied as one of the assumptions behind the method is normally distributed observations.

```{r}
ggpairs(Boston, lower = list(combo = wrap("facethist", bins = 20)))
```

The correlations are better seen from the correlation matrix (below). The crime rate has a slight positive correlation with accessibility to radial highways (rad) and property-tax rate (tax). It is interesting that the access to radial highways and property taxes are strongly correlated with each other. I cannot deduce a possible reason behind this correlations so I leave the explanations to the sociologists and economists.

Strong negative correlation is seen between weighted mean of distances to five Boston employment centres (dis) and proportion of owner-occupied units built prior to 1940 (age). With growing distance the proportion of old houses decreases, which is likely due to the fact that the old houses are concentrated near the center. Newer houses were build further away from the center as the city expanded. Distance is negatively correlated also with concentration of nitrogen oxides (nox), which means the pollution level decreases with growing distance from the city centers. Also the proportion of non-retail business acres per town (indus) decreases further away from the centers. These are really not relevant for the analysis conducted here, so I will not go deeper into the other variables but concentrate on the crime rate.

```{r}
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```


First, the dataset is standardized in order to make the variables comparable when conducting classification. As seen from the summaries below, the standardized values are centered around zero. However, the scaling does not influence the original distribution and thus it can be seen from the histogram that the distribution of the scaled crime rate remains logarithmic.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled = as.data.frame(boston_scaled)

# summary of the scaled crime rate
summary(boston_scaled)
```
```{r}
# plot histogram to see the distribution of scaled crime rate
qplot(boston_scaled$crim, geom="histogram")
```

Next the scaled crime rate is categorized using quantiles as the break points. From the distribution it was already seen that most of the areas in Boston had very low crime rates, making the use of quantiles in categorizing the data questionable. Labeling the categories with equal amount of observations as low, med_low, med_high and high is thus a bit misleading as most of the suburbs have very low crime rate.

```{r}
# create a quantile vector of crim
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Despite the distribution, I go on fitting linear discriminant analysis on the data. Before that the data is divided into train and test sets. The train set consists of 80 % of the observations.

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crim

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

For data classification, the linear discriminant analysis is fitted on the train set with categorical crime rate as the target variable and all the other variables as predictor variables. The fit is shown in the figure below. As can be seen from the figure, the four categories (low, med_low, med_high, high) form two clusters. Of these categories med_high (green) is divided between these two clusters. Therefore, it seems that two categories could be enough to represent the data: low and high crime rates.

```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

The linear discriminant analysis model fitted to the train dataset is used to predict the classes of the test data. The results are shown in the table below with comparison to the correct categories. It is seen that the majority of the results lie on the diagonal, meaning that the prediction matches the original categories rather well. There are no predictions that are completely off as the values in the bottom left and top right corners are zeros. Although the model seems to perform reasonably well, there are some discrepancies. 

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

Because the linear discriminant analysis showed that perhaps only two categories are needed, Euklidean distance is calculated to represent similarities between observations and K-means clustering is applied to investigate the optimal number of categories. From the figure below it is seen that as the curve decreases strongly around value 2, two categories are sufficient to represent the similarities within the data.

```{r}
# reload the Boston dataset
data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled = as.data.frame(boston_scaled)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)
```


```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

As two categories seems to be sufficient to represent the data, the K-means clustering is applied again with two centers. From the figure below it is seen that the centers of the two groups are clearly separated for many of the variables. In the uppermost row red dots represents the group of low crime rate and black dots the group of high crime rate. As many of the variables are beyond my understanding, it is difficult for me to make conclusions of the results. However, for example, crime rate versus proportion of lower status population (lstat) is higher in the areas with high crime rate. Also the median value of owner occupied houses (medv) is lower in the high crime areas. Crime rate seems also in general higher when the distance from centers is low. On the other hand, the proportion of blacks in town does not seem to clearly influence the crime rate (although, this value is originally scaled as 1000(Bk - 0.63)^2 where Bk is the proportion of blacks, and therefore it is not so easy to understand what is going on here).
    
```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster, lower.panel = NULL)
```

Grouping with two centers (above) can be compared to grouping with three centers shown below. With three groups there are some overlapping with centers.

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled[1:5], col = km$cluster, lower.panel = NULL)
```


